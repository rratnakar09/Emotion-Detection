{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Facial Expression Recognition__\n",
    "\n",
    "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories \n",
    "\n",
    "0. Angry, \n",
    "1. Disgust, \n",
    "2. Fear, \n",
    "3. Happy, \n",
    "4. Sad, \n",
    "5. Surprise, \n",
    "6. Neutral\n",
    "\n",
    "Data Link: https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n",
    "\n",
    "I will test the facial expressio on trained model which I have trained in emotion_recognition notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this program will load the model trained in emotion_recognition program\n",
    "# we will test the facial emotion with the video stream\n",
    "# import the dependencies\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = model_from_json(open(\"emotion_det_cnn.json\", \"r\").read())\n",
    "#load weights\n",
    "model.load_weights('emotion_det_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the harr file which will detect face\n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the web cam to stream video\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,test_img=cap.read()# captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    # convert the test image into a gray scale image\n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    # detect the face in this grau image\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, scaleFactor=1.5, minNeighbors=5)\n",
    "\n",
    "\n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        # make a rectangle around the detected face in video stream\n",
    "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "        roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image\n",
    "        # pre process the image to fit with model\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "        img_pixels /= 255\n",
    "\n",
    "        # predict the emotion on the trained model\n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        #find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index] # get the label of emotion\n",
    "\n",
    "        # put a text on the image with label\n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (640, 480))\n",
    "    cv2.imshow('Facial emotion analysis ',resized_img)\n",
    "\n",
    "    if cv2.waitKey(27) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# when everything is done release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
